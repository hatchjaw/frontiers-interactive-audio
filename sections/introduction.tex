\section{Introduction}\label{sec:intro}

Spatial and immersive audio techniques have been the beneficiaries of
significant research interest over the past few decades.
Recent developments in virtual and augmented reality technologies and
\textit{object-based} audio have led to an acceleration in interest in the
creation of virtual sound fields via approaches such as Wave Field Synthesis
(WFS) and Higher Order Ambisonics (HOA)\cite{berkhout_acoustic_1993,
    ahrens_theory_2008,daniel_further_2003,frank_producing_2015}.
These techniques call for the deployment of large numbers of loudspeakers, and
centralised, \textit{in situ} installations of dedicated hardware and software.
The costs associated with such installations have seen them largely restricted
to the preserve of concert venues, cinemas, and institutions with the means to
purchase and operate large-scale systems of this sort.

Advancements in embedded computing mean that there now exist an assortment of
small, low-cost devices with support for audio digital signal processing (DSP).
These devices are open source, relatively easy to program, and may provide
support for communication over ubiquitous computer networking equipment and
protocols.
A network of such devices could be used to \textit{distribute} the problem of
audio spatialisation, potentially lowering the barrier to entry to what is
otherwise a comparatively exclusive branch of audio research.

The work that this article describes is an exploration of the possibility of
achieving such an outcome.
As such, it is concerned first and foremost with the transmission of digital
audio signals throughout a computer network.
It is meaningful, then, to reflect on the nature of such signals, a selection
of their properties of greatest pertinence to this work, and the representation
of audio signals within computer systems and networks.

\subsection{Digital Audio Signals}
\label{subsec:digital-audio-signals}

In a digital audio system with sampling rate $F_s$, an audio signal $y$ is
composed of samples $y[n]$, each representing the amplitude of the signal at a
given point in time $t$, where $t = n/F_s$.
For arithmetical convenience, sample amplitudes are typically treated as
floating point numbers, constrained to the interval $y \in [-1, 1]$ (see
\figref{fig:signal-samples}).
It is in this form that audio samples are typically handled during the
processing stage of a DSP algorithm, and the underlying representation
\textemdash{} that required by low level software and hardware systems
\textemdash{} is concealed; matters of numerical resolution and precision are,
by-and-large, abstracted away.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/digital-signal}
    \caption{
        An audio signal (\qty{1}{\kHz} sine wave);
        (a) in continuous time;
        (b) sampled at intervals of $1/Fs$ seconds, with $F_s=\qty{44.1}{\kHz}$;
        (c) detail of (b), sample amplitudes converted to 16-bit hexadecimal
        values.
    }
    \label{fig:signal-samples}
\end{figure}

Samples undergo format conversion at various stages during processing, such as
from an integer pulse code modulation (PCM) filetype to a stream of floating
point audio samples in a digital audio workstation (DAW), or from a floating
point audio stream in an audio device driver to an integer stream to be handled
by a hardware codec.
For the most part, the user, and even the developer of audio software, need not
concern themselves with the rudiments of sample representation and conversion;
as shall be shown, however, under certain circumstances these fundamental
aspects of digital audio systems must be dealt with directly.

\subsection{Numerical Representation}\label{subsec:numerical-representation}

Digital audio samples are ultimately described as streams of binary numbers.
Broadly speaking, the more binary digits (\textit{bits}) available for each
sample, the greater the resolution in terms of distinct amplitudes that can be
represented, with ramifications for dynamic range and signal-to-noise ratio, as
well as for storage and throughput.
Integer sample formats, such as commonly-encountered 16 and 24-bit, offer
comparatively poor resolution at low amplitudes due to the incongruity between
the linear distribution of their values versus the logarithmic nature of sound
intensity.
Floating point formats, by contrast, feature a logarithmic distribution of
available values; the IEEE standard for single-precision (i.e.\ 32-bit) floating
point numbers~\citep{ieee_ieee_1985} describes numbers over the interval
$\pm[0,\num{3.403e38}]$ but with precision clustered around zero, around half
the available values lying in the interval $[-1, 1]$.

To give a brief example, consider the 16-bit case, and a single 16-bit integer
audio sample:
\begin{equation*}
    \texttt{11010101 01101010}
\end{equation*}
Separating the bits into two groups of eight is reflective of the fact that the
eight-bit \textit{byte} is typically the unit of transmission in computer
systems.
Grouping the bits in this way points to their expression in hexadecimal format,
transforming the bytes into more easily-digestible morsels of two digits apiece:
\begin{equation*}
    \texttt{d5 6a}
\end{equation*}
A number of this kind may also be seen represented (in C and C++ code for
example) as \texttt{0xd56a}, with `\texttt{0x}' indicating that the number to
follow is in base sixteen.
Sixteen bits grant access to $2^{16}=\num{65536}$ distinct amplitude values for
each sample; what the above tells us is that, in decimal terms, this sample
should take the \num{54634}\textsuperscript{th} amplitude value\footnote{
    For convenience and explicitness elsewhere in the text, decimal equivalents
    to hexadecimal numbers will be indicated with subscript 10, e.g.
    \num{54634}\textsubscript{10}.
}.

\subsection{Storage and Transmission}\label{subsec:storage-and-transmission}

The above binary and hexadecimal representations hint at the property of
\textit{endianness}, i.e.\ the order in which a number's component bits and
bytes appear~\citep{cohen_holy_1981}.
The given examples mirror the left-to-right nature of western written language
and numbers, being \textit{big endian} at the levels of both bit and byte, with
the \textit{most significant bit} (and \textit{most significant byte}, both
abbreviated \textit{MSB}) appearing first.

To paraphrase Cohen~\citep{cohen_holy_1981}, if the unit of digital audio
transmission was \textit{an audio signal}, then endianness would not be a
matter of concern.
Practically speaking, however, to be handled by software or hardware, or
transmitted over a network, audio signals must be decomposed into a hierarchy
of temporally-distributed blocks, those blocks into samples, samples into bytes,
and bytes into bits;
correct endianness must be observed with respect to differing computer
architectures, file formats, and transmission protocols.

The PCM WAV file format, for example, dictates that the least significant byte
of each sample is stored first \textemdash{} little endian \textemdash{} but
that bit order should be big endian~\citep{noauthor_multimedia_1991};
thus the hexadecimal number above should be stored in a \texttt{.wav} file as
\texttt{6ad5}.
The ethernet standard for communication over local area computer
networks~\citep{noauthor_ieee_2018}, by contrast, calls for something akin to
the opposite:
the most significant byte is transmitted first, with each byte sent
little-endian;
returning to the binary representation:
\begin{equation*}
    \texttt{11010101 01101010} \quad\rightarrow\quad \texttt{10101011 01010110}
\end{equation*}
Note, however, that network packet capture software such as
Wireshark\footnote{\url{https://www.wireshark.org/}} reports the bytes of
intercepted packets with most significant bit first.


\section{Networked Audio}\label{sec:networked-audio}

\todo[inline]{Less detail on telephony?}
The transmission of audio data has been a topic of research interest since the
earliest days of computer networking as it is recognised today, i.e.\ over
packet-switched networks, whereby data to be transmitted is grouped into
packets, each consisting of a header and a payload.
Voice transmission over ARPANET was being conducted as early as
1974~\citep{schulzrinne_voice_1992} and the first standard for voice
communication over packet-switched networks \textemdash{} the Network Voice
Protocol (NVP) \textemdash{} was released in
1977~\citep{cohen_specifications_1977}.

With its references to `calling' and `ringing', it is clear that the NVP
standard was intended for digital telephony.
Indeed, research on networked audio was primarily concerned with telephony well
into the 1990s, focusing on real-time voice communication over wide area
networks (WAN) with efforts centring on \textit{quality of service} (QoS),
particularly with regard to the perennial issues of latency, packet loss, and
network jitter \textemdash{} inconsistencies in the rate of packet
transmission~\citep{hardman_reliable_1995,hardman_successful_1998}.
Work at this time dealt with streams of compressed audio data, and speech
coding algorithms to overcome the deleterious effects of dropped packets over
unreliable network paths and low-bandwidth connections.

Whereas the priority for digital telephony, and later voice over IP (VoIP)
systems, is intelligibility, for musical purposes fidelity, and the use of
uncompressed audio signals, is of greater concern.
With the increasing availability of high-speed internet connections in the late
1990s came research into transmitting uncompressed audio data over the
internet~\citep{chafe_simplified_2000,xu_real-time_2000}.
Work of this sort was spearheaded by the \textit{SoundWIRE} project, developed
by researchers at McGill University and the Centre for Computer Research in
Music and Acoustics (CCRMA) at Stanford University, and took the form of
a wide variety of experiments with high quality audio over both WAN and local
area networks (LAN).
These experiments included LAN-based real-time musical
performances~\citep{chafe_simplified_2000}, concert streaming over
WAN~\citep{xu_real-time_2000,chafe_simplified_2000}, and sonification of QoS via
a distributed digital waveguide dubbed the
\textit{Network Harp}~\citep{chafe_simplified_2000,chafe_physical_2002}

\subsection{Protocols and Systems}\label{subsec:protocols-and-systems}

VoIP research in the 1990s focused on matters such as audio codecs and data
compression~\citep{turletti_inria_1995,hardman_successful_1998}, seeking a
compromise with the \textit{best-effort} nature of internet service.
The SoundWIRE project, in search of high audio quality, turned its attention
directly to the basic transport layer protocols of the Internet Protocol suite:
Transmission Control Protocol (TCP) and User Datagram Protocol (UDP).
Chafe et al.\ characterised their compression-free system as taking a
``simplified approach''~\citep{chafe_simplified_2000} to networked audio,
emphasising the importance of delivering multichannel audio of at least CD
quality (16-bit, \qty{44.1}{\kHz}) with as little latency as possible.

SoundWIRE experiments included using TCP for unidirectional transmission such as
concert streaming.
TCP is in fact a bidirectional protocol, but its \textit{connection-oriented},
one-to-one design, whereby networked entities establish a connection via a
`handshake', following which packets of data are exchanged, allows for
mechanisms that guarantee packet ordering and provide protections against packet
loss~\citep{schiavoni_alternatives_2013,al-dhief_performance_2018}.
These mechanisms mean that, at the expense of increased latency, quality of
service, and thus audio fidelity, is ensured; ideal for a remote concert
scenario.
%The strict one-to-one nature of TCP clearly places limits on its
%applicability to distributed computing, however.

UDP by comparison is a \textit{connectionless} protocol, providing no guarantees
regarding the integrity of the stream of network data, but equally none of the
computational, or indeed temporal, overhead that such guarantees introduce.
A network entity can send UDP packets to a network address irrespective of
the presence or otherwise of another entity with that address.
Further, \textit{many-to-many} (multicast) and \textit{one-to-many} (broadcast)
modes of transmission are possible via address spaces reserved as part of the
internet protocol standard~\citep{meyer_iana_2010}.
Via UDP, SoundWIRE was able to run as a distributed digital waveguide over a
WAN spanning around \qty{4500}{\km}~\citep{chafe_simplified_2000}.

From the SoundWIRE project emerged
\textit{JackTrip}~\citep{caceres_jacktrip_2010,caceras_jacktripsoundwire_2010},
a hybrid system that couples a TCP handshake with audio transmission over UDP,
thus sidestepping the overhead of TCP packet flow control.
Rather than rely on TCP's built-in mechanisms for stream integrity, JackTrip
supplements UDP with a number of optional buffering strategies that aim to
tailor its use to operation over local versus wide area networks.
In this sense it is more flexible than TCP, but in effect JackTrip moulds UDP
transmission into something akin to the connection-oriented model of TCP, and,
in its `hub server' mode, into a kind of \textit{multiple one-to-one} design
\textemdash{} multicast transmission is not possible.

UDP has emerged as the protocol of choice of platforms enabling remote musical
collaboration, serving as the basis for systems such as
NetJACK~\citep{carot_netjack_2009}, part of the JACK Audio Connection Kit (a
cross-platform audio host), Jamulus~\citep{fischer_case_2015},
Soundjack~\citep{renaud_networked_2012}, and other jamming-focused platforms,
plus more recent entrants such as the closed-source, but ultimately UDP-based
Elk OS~\citep{turchet_elk_2021}.
UDP even plays a fundamental role in proprietary systems such as Dante (Digital
Audio Network Through Ethernet)~\citep{noauthor_what_nodate}.

\subsubsection{AoE in the Audio Industry}

In parallel with the work being carried out by researchers such as those
developing SoundWIRE, JackTrip and NetJACK, audio industry bodies were taking
an interest in networked audio.
Prominent amongst these bodies were the IEEE (Institute of Electrical and
Electronics Engineers) and AES (Audio Engineering Society) standards groups,
and companies like Audinate, the creators of the Dante system.
Traditional large-scale audio systems such as those used in broadcast, concert
venues and recording studios rely on the installation of unwieldy systems of
analogue hardware and cabling, with many potential points of failure.
Seeking literally to lighten the load posed by ``hundreds of
kilograms''~\citep{bakker_introduction_2014} of analogue cabling in analogue
audio installations, in the 2000s industry entities were looking to high speed
ethernet as a means to simplify the provision of high-quality, multichannel
audio in industry settings.

Dante, with its promise of low-latency, highly-multichannel audio over ethernet,
and device synchronisation via Precision Time Protocol (PTP), has become the de
facto standard in this area~\citep{bakker_introduction_2014}.
In 2011, IEEE released the Audio Video Bridging (AVB, IEEE 802.1) standard,
and AES67 followed in 2013; these open technical standards describe operation
at layers below TCP and UDP in the OSI (Open Systems Interconnection)
model~\citep{}\todo[inline]{Add source for OSI}, and provide frameworks for
interoperability between AoE and AoIP systems, including mechanisms for device
discovery and synchronisation, again via PTP\@.
Being standards, and not implementations in themselves, it is then up to
manufacturers to implement the appropriate recommendations in their products.
(AES67, for example, has in fact been implemented in Dante.)

Bakker et al.\ refer to Dante as an ``open'' system.
This is perhaps true in the sense that companies can incorporate the Dante
system into their own products under licence from Audinate, but, from the
perspective of the academic community, Dante is very much a closed-source
initiative and not a suitable platform for research.
Open implementations of AVB and AES67 could be of interest, however their
reliance on PTP, support for which is not offered by ubiquitous, low-cost
networking equipment, raises the barrier to entry to systems based on these
standards.
Ultimately, if an accessible solution is sought, attention must be turned back
to the transport layer, and UDP\@.

\subsection{Anatomy of a Datagram}\label{subsec:anatomy-of-a-datagram}

\begin{figure}[h]
    \centering
    \includegraphics[width=.5\textwidth]{figures/udp}
    \caption{Structure of an ethernet frame containing a UDP packet.}
    \label{fig:udp-frame}
\end{figure}

A UDP packet consists of data encoded in 8-bit integer format.
Being a transport layer protocol, a UDP packet is in fact preceded in an
ethernet frame by information relating to lower layers in the OSI hierarchy: the
network layer and the data link layer.
